{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74688a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the file path\n",
    "output_folder = \"documents\"\n",
    "filename = \"think_python_guide.pdf\"\n",
    "url = \"https://greenteapress.com/thinkpython/thinkpython.pdf\"\n",
    "file_path = Path(output_folder) / filename\n",
    "\n",
    "\n",
    "def download_file(url: str, file_path: Path):\n",
    "    response = requests.get(url, stream=True, timeout=30)\n",
    "    response.raise_for_status()\n",
    "    file_path.write_bytes(response.content)\n",
    "\n",
    "\n",
    "# Download the file if it doesn't exist\n",
    "if not file_path.exists():\n",
    "    download_file(\n",
    "        url=url,\n",
    "        file_path=file_path,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9525bddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 300 characters:\n",
      "Think Python\n",
      "\n",
      "How to Think Like a Computer Scientist\n",
      "\n",
      "Version 2.0.17\n",
      "\n",
      "\f\fThink Python\n",
      "\n",
      "How to Think Like a Computer Scientist\n",
      "\n",
      "Version 2.0.17\n",
      "\n",
      "Allen Downey\n",
      "\n",
      "Green Tea Press\n",
      "\n",
      "Needham, Massachusetts\n",
      "\n",
      "\fCopyright © 2012 Allen Downey.\n",
      "\n",
      "Green Tea Press\n",
      "9 Washburn Ave\n",
      "Needham MA 02492\n",
      "\n",
      "Permission is granted...\n"
     ]
    }
   ],
   "source": [
    "from markitdown import MarkItDown\n",
    "\n",
    "# Initialize the converter\n",
    "md = MarkItDown()\n",
    "\n",
    "# Convert the Python guide to markdown\n",
    "result = md.convert(file_path)\n",
    "python_guide_content = result.text_content\n",
    "\n",
    "# Display the conversion results\n",
    "print(\"First 300 characters:\")\n",
    "print(python_guide_content[:300] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96927336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ready: 460,251 characters\n"
     ]
    }
   ],
   "source": [
    "# Organize the converted document\n",
    "processed_document = {\n",
    "    'source': file_path,\n",
    "    'content': python_guide_content\n",
    "}\n",
    "\n",
    "# Create a list containing our single document for consistency with downstream processing\n",
    "documents = [processed_document]\n",
    "\n",
    "# Document is now ready for chunking and embedding\n",
    "print(f\"Document ready: {len(processed_document['content']):,} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533b71b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danya\\Desktop\\projects\\rag_pipline\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 336 chars → 3 chunks\n",
      "Chunk 1: Machine learning transforms data processing. It enables pattern recognition without explicit programming.\n",
      "Chunk 2: Deep learning uses neural networks with multiple layers. These networks discover complex patterns automatically.\n",
      "Chunk 3: Natural language processing combines ML with linguistics. It helps computers understand human language effectively.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d96d83e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=600,         # Optimal chunk size for Q&A scenarios\n",
    "    chunk_overlap=120,      # 20% overlap to preserve context\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]  # Split hierarchy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4055a341",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(doc, text_splitter):\n",
    "    \"\"\"Process a single document into chunks.\"\"\"\n",
    "    doc_chunks = text_splitter.split_text(doc[\"content\"])\n",
    "    return [{\"content\": chunk, \"source\": doc[\"source\"]} for chunk in doc_chunks]\n",
    "\n",
    "\n",
    "# Process all documents and create chunks\n",
    "all_chunks = []\n",
    "for doc in documents:\n",
    "    doc_chunks = process_document(doc, text_splitter)\n",
    "    all_chunks.extend(doc_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a592d3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 1007\n",
      "Chunk length: 68-598 characters\n",
      "Source document: think_python_guide.pdf\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "source_counts = Counter(chunk[\"source\"] for chunk in all_chunks)\n",
    "chunk_lengths = [len(chunk[\"content\"]) for chunk in all_chunks]\n",
    "\n",
    "print(f\"Total chunks created: {len(all_chunks)}\")\n",
    "print(f\"Chunk length: {min(chunk_lengths)}-{max(chunk_lengths)} characters\")\n",
    "print(f\"Source document: {Path(documents[0]['source']).name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af652c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danya\\Desktop\\projects\\rag_pipline\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\danya\\.cache\\huggingface\\hub\\models--sentence-transformers--multi-qa-mpnet-base-dot-v1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation results:\n",
      "  - Embeddings shape: (1007, 768)\n",
      "  - Vector dimensions: 768\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load Q&A-optimized embedding model (downloads automatically on first use)\n",
    "model = SentenceTransformer('multi-qa-mpnet-base-dot-v1')\n",
    "\n",
    "# Extract documents and create embeddings\n",
    "documents = [chunk[\"content\"] for chunk in all_chunks]\n",
    "embeddings = model.encode(documents)\n",
    "\n",
    "print(f\"Embedding generation results:\")\n",
    "print(f\"  - Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"  - Vector dimensions: {embeddings.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4144a23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test how one query finds relevant Python programming content\n",
    "from sentence_transformers import util\n",
    "\n",
    "query = \"How do you define functions in Python?\"\n",
    "document_chunks = [\n",
    "    \"Variables store data values that can be used later in your program.\",\n",
    "    \"A function is a block of code that performs a specific task when called.\",\n",
    "    \"Loops allow you to repeat code multiple times efficiently.\",\n",
    "    \"Functions can accept parameters and return values to the calling code.\"\n",
    "]\n",
    "\n",
    "# Encode query and documents\n",
    "query_embedding = model.encode(query)\n",
    "doc_embeddings = model.encode(document_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82f52d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'How do you define functions in Python?'\n",
      "Document chunks ranked by relevance:\n",
      "1. (0.674): 'A function is a block of code that performs a specific task when called.'\n",
      "2. (0.607): 'Functions can accept parameters and return values to the calling code.'\n",
      "3. (0.461): 'Loops allow you to repeat code multiple times efficiently.'\n",
      "4. (0.448): 'Variables store data values that can be used later in your program.'\n"
     ]
    }
   ],
   "source": [
    "# Calculate similarities using SentenceTransformers util\n",
    "similarities = util.cos_sim(query_embedding, doc_embeddings)[0]\n",
    "\n",
    "# Create ranked results\n",
    "ranked_results = sorted(\n",
    "    zip(document_chunks, similarities), \n",
    "    key=lambda x: x[1], \n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "print(f\"Query: '{query}'\")\n",
    "print(\"Document chunks ranked by relevance:\")\n",
    "for i, (chunk, score) in enumerate(ranked_results, 1):\n",
    "    print(f\"{i}. ({score:.3f}): '{chunk}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f549a1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created collection: python_guide\n",
      "Collection ID: 25bd03d2-dbbd-4127-be76-fdcba4767ee7\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "# Create persistent client for data storage\n",
    "client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "# Create collection for business documents (or get existing)\n",
    "collection = client.get_or_create_collection(\n",
    "    name=\"python_guide\",\n",
    "    metadata={\"description\": \"Python programming guide\"}\n",
    ")\n",
    "\n",
    "print(f\"Created collection: {collection.name}\")\n",
    "print(f\"Collection ID: {collection.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a791fcdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection count: 1007\n"
     ]
    }
   ],
   "source": [
    "# Prepare metadata and add documents to collection\n",
    "metadatas = [{\"document\": Path(chunk[\"source\"]).name} for chunk in all_chunks]\n",
    "\n",
    "collection.add(\n",
    "    documents=documents,\n",
    "    embeddings=embeddings.tolist(), # Convert numpy array to list\n",
    "    metadatas=metadatas, # Metadata for each document\n",
    "    ids=[f\"doc_{i}\" for i in range(len(documents))], # Unique identifiers for each document\n",
    ")\n",
    "\n",
    "print(f\"Collection count: {collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a11933d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_query_results(question, query_embedding, documents, metadatas):\n",
    "    \"\"\"Format and print the search results with similarity scores\"\"\"\n",
    "    from sentence_transformers import util\n",
    "\n",
    "    print(f\"Question: {question}\\n\")\n",
    "\n",
    "    for i, doc in enumerate(documents):\n",
    "        # Calculate accurate similarity using sentence-transformers util\n",
    "        doc_embedding = model.encode([doc])\n",
    "        similarity = util.cos_sim(query_embedding, doc_embedding)[0][0].item()\n",
    "        source = metadatas[i].get(\"document\", \"Unknown\")\n",
    "\n",
    "        print(f\"Result {i+1} (similarity: {similarity:.3f}):\")\n",
    "        print(f\"Document: {source}\")\n",
    "        print(f\"Content: {doc[:300]}...\")\n",
    "        print()\n",
    "\n",
    "\n",
    "def query_knowledge_base(question, n_results=2):\n",
    "    \"\"\"Query the knowledge base with natural language\"\"\"\n",
    "    # Encode the query using our SentenceTransformer model\n",
    "    query_embedding = model.encode([question])\n",
    "\n",
    "    results = collection.query(\n",
    "        query_embeddings=query_embedding.tolist(),\n",
    "        n_results=n_results,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"],\n",
    "    )\n",
    "\n",
    "    # Extract results and format them\n",
    "    documents = results[\"documents\"][0]\n",
    "    metadatas = results[\"metadatas\"][0]\n",
    "\n",
    "    format_query_results(question, query_embedding, documents, metadatas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fab8b2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Initialize the local LLM\n",
    "llm = OllamaLLM(model=\"llama3.2:latest\", temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0cc794be",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"You are a Python programming expert. Based on the provided documentation, answer the question clearly and accurately.\n",
    "\n",
    "Documentation:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer (be specific about syntax, keywords, and provide examples when helpful):\"\"\"\n",
    ")\n",
    "\n",
    "# Create the processing chain\n",
    "chain = prompt_template | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b858d028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context(question, n_results=5):\n",
    "    \"\"\"Retrieve relevant context using embeddings\"\"\"\n",
    "    query_embedding = model.encode([question])\n",
    "    results = collection.query(\n",
    "        query_embeddings=query_embedding.tolist(),\n",
    "        n_results=n_results,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"],\n",
    "    )\n",
    "\n",
    "    documents = results[\"documents\"][0]\n",
    "    context = \"\\n\\n---SECTION---\\n\\n\".join(documents)\n",
    "    return context, documents\n",
    "\n",
    "\n",
    "def get_llm_answer(question, context):\n",
    "    \"\"\"Generate answer using retrieved context\"\"\"\n",
    "    answer = chain.invoke(\n",
    "        {\n",
    "            \"context\": context[:2000],\n",
    "            \"question\": question,\n",
    "        }\n",
    "    )\n",
    "    return answer\n",
    "\n",
    "\n",
    "def format_response(question, answer, source_chunks):\n",
    "    \"\"\"Format the final response with sources\"\"\"\n",
    "    response = f\"**Question:** {question}\\n\\n\"\n",
    "    response += f\"**Answer:** {answer}\\n\\n\"\n",
    "    response += \"**Sources:**\\n\"\n",
    "\n",
    "    for i, chunk in enumerate(source_chunks[:3], 1):\n",
    "        preview = chunk[:100].replace(\"\\n\", \" \") + \"...\"\n",
    "        response += f\"{i}. {preview}\\n\"\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "def enhanced_query_with_llm(question, n_results=5):\n",
    "    \"\"\"Query function combining retrieval with LLM generation\"\"\"\n",
    "    context, documents = retrieve_context(question, n_results)\n",
    "    answer = get_llm_answer(question, context)\n",
    "    return format_response(question, answer, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "128b843e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Question:** How do if-else statements work in Python?\n",
      "\n",
      "**Answer:** If-else statements in Python are used for conditional execution of code. Here's a breakdown of how they work:\n",
      "\n",
      "**Syntax**\n",
      "\n",
      "The basic syntax of an if-else statement in Python is as follows:\n",
      "```\n",
      "if condition:\n",
      "    # code to execute if condition is true\n",
      "elif condition2:\n",
      "    # code to execute if condition1 is false and condition2 is true\n",
      "else:\n",
      "    # code to execute if both conditions are false\n",
      "```\n",
      "**Keywords**\n",
      "\n",
      "The keywords used in an if-else statement are:\n",
      "\n",
      "* `if`: used to check a condition\n",
      "* `elif` (short for \"else if\"): used to check another condition if the first one is false\n",
      "* `else`: used to specify code to execute if all conditions are false\n",
      "\n",
      "**How it works**\n",
      "\n",
      "Here's how an if-else statement works in Python:\n",
      "\n",
      "1. The interpreter evaluates the condition inside the `if` block.\n",
      "2. If the condition is true, the code inside the `if` block is executed.\n",
      "3. If the condition is false, the interpreter moves on to the next line and checks the condition inside the `elif` block.\n",
      "4. If the `elif` condition is true, the code inside that block is executed.\n",
      "5. If both conditions are false, the code inside the `else` block is executed.\n",
      "\n",
      "**Example**\n",
      "\n",
      "Here's an example of a simple if-else statement:\n",
      "```\n",
      "x = 5\n",
      "if x > 10:\n",
      "    print(\"x is greater than 10\")\n",
      "elif x == 5:\n",
      "    print(\"x is equal to 5\")\n",
      "else:\n",
      "    print(\"x is less than 5\")\n",
      "```\n",
      "In this example, the condition `x > 10` is false, so the interpreter moves on to the next line and checks the condition `x == 5`. Since this condition is true, the code inside that block is executed. If both conditions were false, the code inside the `else` block would be executed.\n",
      "\n",
      "**Chained Conditionals**\n",
      "\n",
      "Python also supports chained conditionals, which allow you to check multiple conditions in a single statement:\n",
      "```\n",
      "if x < y and y < z:\n",
      "    print(\"x is less than y and y is less than z\")\n",
      "elif x > y or z > 10:\n",
      "    print(\"x is greater than y or z is greater than 10\")\n",
      "else:\n",
      "    print(\"x is not less than y and z is not greater than 10\")\n",
      "```\n",
      "In this example, the conditions `x < y` and `y < z` are both true, so the code inside that block is executed. If either of these conditions were false, the interpreter would move on to the next line and check the condition `x > y or z > 10`.\n",
      "\n",
      "**Sources:**\n",
      "1. 5.6 Chained conditionals  Sometimes there are more than two possibilities and we need more than two ...\n",
      "2. 5. An unclosed opening operator—(, {, or [—makes Python continue with the next line as part of the c...\n",
      "3. if x == y:  print  else:  ’  x and y are equal  ’  if x < y:  \f44  Chapter 5. Conditionals and recur...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "enhanced_response = enhanced_query_with_llm(\"How do if-else statements work in Python?\")\n",
    "print(enhanced_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1eeb1cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_llm_answer(question, context):\n",
    "    \"\"\"Stream LLM answer generation token by token\"\"\"\n",
    "    for chunk in chain.stream({\n",
    "        \"context\": context[:2000],\n",
    "        \"question\": question,\n",
    "    }):\n",
    "        yield getattr(chunk, \"content\", str(chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "feff1b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are Python loops?\n",
      "Answer: Python loops are control structures that allow you to execute a block of code repeatedly for each item in a sequence. There are two main types of loops in Python:\n",
      "\n",
      "1. **For Loops**\n",
      "\n",
      "A for loop is used to iterate over a sequence (such as a list, tuple, or string) and perform an action on each item in the sequence.\n",
      "\n",
      "Syntax:\n",
      "```python\n",
      "for variable in sequence:\n",
      "    # code to be executed\n",
      "```\n",
      "Example:\n",
      "```python\n",
      "fruits = ['apple', 'banana', 'cherry']\n",
      "for fruit in fruits:\n",
      "    print(fruit)\n",
      "```\n",
      "Output:\n",
      "```\n",
      "apple\n",
      "banana\n",
      "cherry\n",
      "```\n",
      "In this example, the `for` loop iterates over the `fruits` list and assigns each item to the variable `fruit`. The code inside the loop is executed for each item in the sequence.\n",
      "\n",
      "2. **While Loops**\n",
      "\n",
      "A while loop is used to execute a block of code repeatedly as long as a certain condition is true.\n",
      "\n",
      "Syntax:\n",
      "```python\n",
      "while condition:\n",
      "    # code to be executed\n",
      "```\n",
      "Example:\n",
      "```python\n",
      "i = 0\n",
      "while i < 5:\n",
      "    print(i)\n",
      "    i += 1\n",
      "```\n",
      "Output:\n",
      "```\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "```\n",
      "In this example, the `while` loop continues to execute as long as the condition `i < 5` is true. The code inside the loop is executed for each iteration.\n",
      "\n",
      "Additionally, Python also has a **for-each** loop (also known as an iterator) which allows you to iterate over a sequence without having to keep track of the index:\n",
      "```python\n",
      "numbers = [1, 2, 3]\n",
      "for num in numbers:\n",
      "    print(num)\n",
      "```\n",
      "This is equivalent to using `range(len(numbers))` and indexing into the list, but it's often more readable and efficient.\n",
      "\n",
      "It's worth noting that Python also has a built-in function called `sum()` which can be used to calculate the sum of all items in a sequence, as shown in the documentation:\n",
      "```python\n",
      "numbers = [1, 2, 3]\n",
      "print(sum(numbers))  # prints 6\n",
      "```\n",
      "This is often more concise and efficient than using a for loop to calculate the sum."
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Test the streaming functionality\n",
    "question = \"What are Python loops?\"\n",
    "context, documents = retrieve_context(question, n_results=3)\n",
    "\n",
    "print(\"Question:\", question)\n",
    "print(\"Answer: \", end=\"\", flush=True)\n",
    "\n",
    "# Stream the answer token by token\n",
    "for token in stream_llm_answer(question, context):\n",
    "    print(token, end=\"\", flush=True)\n",
    "    time.sleep(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c5c1211b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "\n",
    "def rag_interface(question):\n",
    "    \"\"\"Gradio interface reusing existing format_response function\"\"\"\n",
    "    if not question.strip():\n",
    "        yield \"Please enter a question.\"\n",
    "        return\n",
    "\n",
    "    # Use modular retrieval and streaming\n",
    "    context, documents = retrieve_context(question, n_results=5)\n",
    "\n",
    "    response_start = f\"**Question:** {question}\\n\\n**Answer:** \"\n",
    "    answer = \"\"\n",
    "\n",
    "    # Stream the answer progressively\n",
    "    for token in stream_llm_answer(question, context):\n",
    "        answer += token\n",
    "        yield response_start + answer\n",
    "\n",
    "    # Use existing formatting function for final response\n",
    "    yield format_response(question, answer, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8dbc21a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://bb4bb954556591c7a5.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://bb4bb954556591c7a5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset file at: .gradio\\flagged\\dataset1.csv\n"
     ]
    }
   ],
   "source": [
    "# Create Gradio interface with streaming support\n",
    "demo = gr.Interface(\n",
    "    fn=rag_interface,\n",
    "    inputs=gr.Textbox(\n",
    "        label=\"Ask a question about Python programming\",\n",
    "        placeholder=\"How do if-else statements work in Python?\",\n",
    "        lines=2,\n",
    "    ),\n",
    "    outputs=gr.Markdown(label=\"Answer\"),\n",
    "    title=\"Intelligent Document Q&A System\",\n",
    "    description=\"Ask questions about Python programming concepts and get instant answers with source citations.\",\n",
    "    examples=[\n",
    "        \"How do if-else statements work in Python?\",\n",
    "        \"What are the different types of loops in Python?\",\n",
    "        \"How do you handle errors in Python?\",\n",
    "    ],\n",
    "    # allow_flagging=\"never\"\n",
    ")\n",
    "\n",
    "# Launch the interface with queue enabled for streaming\n",
    "if __name__ == \"__main__\":\n",
    "    demo.queue().launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
